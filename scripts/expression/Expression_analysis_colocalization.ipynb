{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac842b8f-a654-4782-9b2b-f0114eff5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "from itertools import combinations\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ed9f5-9202-40d1-914c-f8660e231027",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/lab/wengpj01/expression_pipeline/results_20231031/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b959f67-43b4-4045-8712-480b3eaf6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_file = \"/lab/wengpj01/expression_pipeline/results_20231031/axolotl_norm.csv\"\n",
    "bull_file = \"/lab/wengpj01/expression_pipeline/results_20231031/bullfrog_norm.csv\"\n",
    "cane_file = \"/lab/wengpj01/expression_pipeline/results_20231031/cane_norm.csv\"\n",
    "terr_file = \"/lab/wengpj01/expression_pipeline/results_20231031/terribilis_norm.csv\"\n",
    "xen_file = \"/lab/wengpj01/expression_pipeline/results_20231031/xenopus_norm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82429047-1137-4e9a-8dea-a4cd5aef11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, specifying the second column as the index (row names) and skipping the first column\n",
    "ax = pd.read_csv(ax_file, index_col=0, usecols=lambda column: column != 'Unnamed: 0')\n",
    "bull = pd.read_csv(bull_file, index_col=0, usecols=lambda column: column != 'Unnamed: 0')\n",
    "cane = pd.read_csv(cane_file, index_col=0, usecols=lambda column: column != 'Unnamed: 0')\n",
    "terr = pd.read_csv(terr_file, index_col=0, usecols=lambda column: column != 'Unnamed: 0')\n",
    "xen = pd.read_csv(xen_file, index_col=0, usecols=lambda column: column != 'Unnamed: 0')\n",
    "\n",
    "# # Rename the columns based on the first row\n",
    "# ax.columns = ax.iloc[0]\n",
    "\n",
    "# Drop the first row (since it's now the column names)\n",
    "# ax = ax.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e0b38-1e5f-4077-abce-2b36d277c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the counts for each column\n",
    "non_zero_counts = {}\n",
    "\n",
    "# Iterate through each column in the DataFrame\n",
    "for column in bull.columns:\n",
    "    # Calculate the number of non-zero values in the column\n",
    "    non_zero_count = (bull[column] != 0).sum()\n",
    "    \n",
    "    # Store the count in the dictionary with the column name as the key\n",
    "    non_zero_counts[column] = non_zero_count\n",
    "\n",
    "# Convert the dictionary to a DataFrame if needed\n",
    "non_zero_counts_bull = pd.DataFrame(list(non_zero_counts.items()), columns=['Column', 'NonZeroRowCount'])\n",
    "\n",
    "# Create a dictionary to store the counts for each column\n",
    "count_bull_10 = {}\n",
    "\n",
    "# Iterate through each column in the bull DataFrame\n",
    "for column in bull.columns:\n",
    "    # Calculate the number of rows where the value is greater than 10\n",
    "    count = (bull[column] > 10).sum()\n",
    "    \n",
    "    # Store the count in the dictionary with the column name as the key\n",
    "    count_bull_10[column] = count\n",
    "\n",
    "# Convert the dictionary to a DataFrame if needed\n",
    "count_bull_10_df = pd.DataFrame(list(count_bull_10.items()), columns=['Column', 'RowCountGreaterThan10'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8b288-5348-4ceb-afbb-3df2105c59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_zero_values(dataframes, dataframe_names):\n",
    "    summary_data = []\n",
    "    \n",
    "    # Define regular expression patterns for each group of columns\n",
    "    pattern_1 = r'^Counts_1\\..*'\n",
    "    pattern_2 = r'^Counts_2\\..*'\n",
    "    pattern_3 = r'^Counts_3\\..*'\n",
    "    \n",
    "    for df in dataframes:\n",
    "        # Find columns that match each pattern in the current DataFrame\n",
    "        matching_columns_1 = [col for col in df.columns if re.match(pattern_1, col)]\n",
    "        matching_columns_2 = [col for col in df.columns if re.match(pattern_2, col)]\n",
    "        matching_columns_3 = [col for col in df.columns if re.match(pattern_3, col)]\n",
    "\n",
    "        # Create boolean masks for each group of matching columns\n",
    "        mask_1 = df[matching_columns_1] != 0\n",
    "        mask_2 = df[matching_columns_2] != 0\n",
    "        mask_3 = df[matching_columns_3] != 0\n",
    "\n",
    "        # Check if there is at least one non-zero value in each group of matching columns for each row\n",
    "        rows_with_non_zero_1 = mask_1.any(axis=1)\n",
    "        rows_with_non_zero_2 = mask_2.any(axis=1)\n",
    "        rows_with_non_zero_3 = mask_3.any(axis=1)\n",
    "\n",
    "        # Calculate the counts for each group of matching columns\n",
    "        count_rows_with_non_zero_1 = rows_with_non_zero_1.sum()\n",
    "        count_rows_with_non_zero_2 = rows_with_non_zero_2.sum()\n",
    "        count_rows_with_non_zero_3 = rows_with_non_zero_3.sum()\n",
    "        \n",
    "        # Count the number of rows in Counts_1\n",
    "        count_rows_in_counts_1 = df[matching_columns_1].shape[0]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Counts_1': count_rows_with_non_zero_1,\n",
    "            'Counts_2': count_rows_with_non_zero_2,\n",
    "            'Counts_3': count_rows_with_non_zero_3,\n",
    "            'Total': count_rows_in_counts_1\n",
    "        })\n",
    "\n",
    "    # Create a summary DataFrame with the provided names as index\n",
    "    summary_df2 = pd.DataFrame(summary_data, index=dataframe_names)\n",
    "    \n",
    "    # Create summary_df4 with the desired columns\n",
    "    summary_df4 = pd.DataFrame()\n",
    "    summary_df4['Counts_1_Percentage'] = (summary_df2['Counts_1'] / summary_df2['Total']) * 100\n",
    "    summary_df4['Counts_2_Percentage'] = (summary_df2['Counts_2'] / summary_df2['Total']) * 100\n",
    "    summary_df4['Counts_3_Percentage'] = (summary_df2['Counts_3'] / summary_df2['Total']) * 100\n",
    "\n",
    "    # Copy the index from summary_df2 to summary_df4\n",
    "    summary_df4.index = summary_df2.index\n",
    "    \n",
    "    return summary_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5d885-3668-43c5-9b0d-8df8962bb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = count_non_zero_values([ax, bull, cane, terr, xen],[\"axolotl\", \"bullfrog\", \"cane\", \"dart\", \"clawed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc504981-cdea-4eeb-9ba2-9ba15de8c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tissues(columns):\n",
    "    tongue_columns = [col for col in columns if re.match(r'^Counts_1\\.1$|^Counts_2\\.1$|^Counts_3\\.1$', col)]\n",
    "    non_tongue_columns = [col for col in columns if col not in tongue_columns]\n",
    "    return tongue_columns, non_tongue_columns\n",
    "\n",
    "def count_non_zero_values(dataframes, dataframe_names):\n",
    "    summary_data = []\n",
    "    \n",
    "    for df in dataframes:\n",
    "        tongue_columns, non_tongue_columns = categorize_tissues(df.columns)\n",
    "        \n",
    "        # Check if there is expression in tongue tissues for each row\n",
    "        expressed_in_tongue = (df[tongue_columns] != 0).any(axis=1)\n",
    "        \n",
    "        # Check if there is expression in non-tongue tissues for each row\n",
    "        expressed_in_non_tongue = (df[non_tongue_columns] != 0).any(axis=1)\n",
    "        \n",
    "        # Calculate counts for each category\n",
    "        count_tongue = expressed_in_tongue.sum()\n",
    "        count_non_tongue = ((~expressed_in_tongue) & expressed_in_non_tongue).sum()\n",
    "        count_not_expressed = (~(expressed_in_tongue | expressed_in_non_tongue)).sum()\n",
    "        \n",
    "        # Count the number of rows in each category\n",
    "        count_total = len(df)\n",
    "\n",
    "        summary_data.append({\n",
    "            'Tongue': count_tongue,\n",
    "            'Non-Tongue': count_non_tongue,\n",
    "            'Not Expressed': count_not_expressed,\n",
    "            'Total': count_total\n",
    "        })\n",
    "\n",
    "    # Create a summary DataFrame with the provided names as index\n",
    "    summary_df = pd.DataFrame(summary_data, index=dataframe_names)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = summary_df[['Tongue', 'Non-Tongue', 'Not Expressed']].copy()\n",
    "    \n",
    "    return plot_data\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "dataframes = [ax, bull, cane, terr, xen]\n",
    "dataframe_names = ['Axolotl', 'Bullfrog', 'Cane', 'Terribilis', 'Xenopus']\n",
    "\n",
    "plot_data = count_non_zero_values(dataframes, dataframe_names)\n",
    "colors = ['#D55E00', '#166A53', '#9A9F9B']  # First color: D55E00, Second color: 166A53, Third color: 9A9F9B\n",
    "\n",
    "plot_data.plot(kind='bar', stacked=True, figsize=(10, 6), color=colors)\n",
    "plt.title('Expression of Receptors in Different Tissue Types')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tissue Type')\n",
    "plt.grid(False)  # Remove gridlines\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/barplots_2024.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467df33-48ba-44eb-9771-8d3fa9fa0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom color palette\n",
    "custom_palette = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "\n",
    "# Extract the data from the summary DataFrame\n",
    "data = summary_df.values  # Assuming summary_df contains the data\n",
    "\n",
    "# Number of replicates\n",
    "num_replicates = data.shape[1]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax2 = plt.subplots()\n",
    "\n",
    "# Set the x-axis positions for the species\n",
    "x_positions = np.arange(len(summary_df.index))\n",
    "\n",
    "# Create the 1D scatterplot with stacked replicates\n",
    "for replicate in range(num_replicates):\n",
    "    y_values = data[:, replicate]\n",
    "    ax2.scatter(x_positions, y_values, label=f'Replicate {replicate + 1}', color=custom_palette)\n",
    "    \n",
    "# Set the y-axis limits\n",
    "ax2.set_ylim(0, 110)\n",
    "\n",
    "# Set labels and title\n",
    "ax2.set_xlabel('Species')\n",
    "ax2.set_ylabel('Values')\n",
    "ax2.set_title('1D Scatterplot of Species Data with Stacked Replicates')\n",
    "\n",
    "# Set x-axis labels to be species names\n",
    "ax2.set_xticks(x_positions)\n",
    "ax2.set_xticklabels(summary_df.index)\n",
    "\n",
    "\n",
    "# Add a horizontal line for the mean\n",
    "mean_values = np.mean(data, axis=1)\n",
    "bin_width = 1 / 4  # Adjust as needed\n",
    "for i, mean_value in enumerate(mean_values):\n",
    "    x_start = x_positions[i] - bin_width / 2\n",
    "    x_end = x_positions[i] + bin_width / 2\n",
    "    ax2.hlines(y=mean_value, xmin=x_start, xmax=x_end, color=custom_palette[i], linestyle='--', label='Mean Value')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/expressed_receptors_tissue_replicate.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae584c-ecec-4ad4-a5fe-19795ede358f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77387d-2b19-4227-a1c7-b0616e53f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their associated names\n",
    "dataframes = [ax, bull, cane, terr, xen]  # Assuming you have these dataframes\n",
    "dataframe_names = ['ax', 'bull', 'cane', 'terr', 'xen']\n",
    "\n",
    "# Initialize empty DataFrames for each replicate\n",
    "replicate_1 = pd.DataFrame(index=dataframe_names, columns=[f'1.{i}' for i in range(1, 8)])\n",
    "replicate_2 = pd.DataFrame(index=dataframe_names, columns=[f'2.{i}' for i in range(1, 8)])\n",
    "replicate_3 = pd.DataFrame(index=dataframe_names, columns=[f'3.{i}' for i in range(1, 8)])\n",
    "\n",
    "# Loop through dataframes and populate replicate DataFrames\n",
    "for i, df in enumerate(dataframes):\n",
    "    # Count the number of rows greater than 0 for each column\n",
    "    count_replicate_1 = (df.filter(like='1.').gt(0).sum()).values\n",
    "    count_replicate_2 = (df.filter(like='2.').gt(0).sum()).values\n",
    "    count_replicate_3 = (df.filter(like='3.').gt(0).sum()).values\n",
    "\n",
    "    # Update the respective replicate DataFrame\n",
    "    replicate_1.loc[dataframe_names[i]] = count_replicate_1\n",
    "    replicate_2.loc[dataframe_names[i]] = count_replicate_2\n",
    "    replicate_3.loc[dataframe_names[i]] = count_replicate_3\n",
    "\n",
    "# Convert the count values to integers\n",
    "replicate_1 = replicate_1.astype(int)\n",
    "replicate_2 = replicate_2.astype(int)\n",
    "replicate_3 = replicate_3.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6627d-a9c6-4180-8301-4321ae4ad76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataframes is a list containing your DataFrames\n",
    "dataframes = [ax, bull, cane, terr, xen]\n",
    "unique_not_tongue = pd.DataFrame(index=['ax', 'bull', 'cane', 'terr', 'xen'], columns=['Count'])\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    criteria = (df['Counts_1.1'] < 0.01) & (df['Counts_2.1'] < 0.01) & (df['Counts_3.1'] < 0.01)\n",
    "    \n",
    "    # Check if the average of specific columns is greater than 0.01\n",
    "    other_column_criteria = ((df.drop(['Counts_1.1', 'Counts_2.1', 'Counts_3.1'], axis=1) > 0.01).any(axis=1))\n",
    "    \n",
    "    count_rows = len(df[criteria & other_column_criteria])\n",
    "    \n",
    "    unique_not_tongue.at[unique_not_tongue.index[i], 'Count'] = count_rows\n",
    "\n",
    "\n",
    "# Print or use the unique_not_tongue DataFrame as needed\n",
    "print(unique_not_tongue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8caaa-83af-4a1b-8a65-35370a18af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp=ax[(ax['Counts_1.1'] < 0.01) & (ax['Counts_2.1'] < 0.01) & (ax['Counts_3.1'] < 0.01)][(ax.drop(['Counts_1.1', 'Counts_2.1', 'Counts_3.1'], axis=1) > 0.01).any(axis=1)]\n",
    "temp=xen[(xen['Counts_1.1'] < 0.01) & (xen['Counts_2.1'] < 0.01) & (xen['Counts_3.1'] < 0.01)][(xen.drop(['Counts_1.1', 'Counts_2.1', 'Counts_3.1'], axis=1) > 0.01).any(axis=1)]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6dbbac-6131-45d3-9daf-5c3b95704ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of Species Data with Stacked Replicates by tissue\n",
    "# Dataframes containing the counts\n",
    "replicates = [replicate_1, replicate_2, replicate_3]\n",
    "\n",
    "# Species names\n",
    "species_names = replicate_1.index\n",
    "\n",
    "# Define colors for species clusters\n",
    "colors = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax2 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Names for the columns in replicate_1\n",
    "column_names = [\"Tongue\", \"Brain\", \"Stomach\", \"Intestines\", \"VSkin\", \"DSkin\", \"Liver\"]\n",
    "\n",
    "# Width of each column within a cluster\n",
    "column_width = 0.1\n",
    "\n",
    "# Loop through each column name and create scatter plots for each species cluster\n",
    "for i, column_name in enumerate(column_names):\n",
    "    for j, species in enumerate(species_names):\n",
    "        # Get the data for the current species and column\n",
    "        data = [replicates[k].loc[species][i] for k in range(3)]\n",
    "        \n",
    "        # Calculate x positions for the scatter points\n",
    "        x_positions_cluster = np.arange(len(species_names)) + i * column_width - (0.5 * column_width)\n",
    "        x_positions_point = [x_positions_cluster[j],x_positions_cluster[j],x_positions_cluster[j]]\n",
    "        \n",
    "        # Create scatter plots\n",
    "        ax2.scatter(x_positions_point, data, label=column_name, color=colors[j])\n",
    "\n",
    "# Set labels and title\n",
    "ax2.set_xlabel('Species')\n",
    "ax2.set_ylabel('Number of TAS2Rs expressed')\n",
    "ax2.set_title('1D Scatterplot of Species Data with Stacked Replicates')\n",
    "\n",
    "# Set x-axis labels to be species names\n",
    "ax2.set_xticks(np.arange(len(species_names)))\n",
    "ax2.set_xticklabels(species_names)\n",
    "\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/expressed_receptors_replicate.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8ac20-f8d3-426e-a171-466da312ce6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd3445-6b72-4a68-b808-038d42397c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_above_threshold(df, threshold):\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate through rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if any value in the row is greater than or equal to the threshold\n",
    "        if (row >= threshold).any():\n",
    "            counter += 1\n",
    "\n",
    "    # Calculate the fraction of rows meeting the condition\n",
    "    total_rows = len(df)\n",
    "    fraction = 100* round(counter / total_rows,2)\n",
    "\n",
    "    return counter, fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6acb25-4b82-49e0-84e5-1e7cd6ebf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_above_threshold_min1(df3, threshold):\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate through rows in the DataFrame\n",
    "    for index, row in df3.iterrows():\n",
    "        # Initialize a dictionary to count occurrences of each suffix\n",
    "        suffix_counts = {}\n",
    "\n",
    "        # Iterate through columns in the row\n",
    "        for colname in row.index:\n",
    "            # Check if the value is greater than or equal to the threshold\n",
    "            if row[colname] >= threshold:\n",
    "                # Extract the suffix (e.g., '.2') from the column name\n",
    "                suffix = colname.split('.')[-1]\n",
    "\n",
    "                # Increment the count for this suffix in the dictionary\n",
    "                suffix_counts[suffix] = suffix_counts.get(suffix, 0) + 1\n",
    "\n",
    "        # Check if there is at least one occurrences of any suffix\n",
    "        if any(count >= 1 for count in suffix_counts.values()):\n",
    "            counter += 1\n",
    "\n",
    "    # Calculate the fraction of rows meeting the condition\n",
    "    total_rows = len(df3)\n",
    "    fraction = 100 * round(counter / total_rows, 4)\n",
    "    \n",
    "    counter = int(counter)\n",
    "\n",
    "    return counter, fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061265a-38ed-4c5f-a1e6-6cf6e279d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rows_above_threshold_min1(bull, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c24d2-e9ff-444b-9f3d-9759704b143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_above_threshold_min2(df2, threshold):\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate through rows in the DataFrame\n",
    "    for index, row in df2.iterrows():\n",
    "        # Initialize a dictionary to count occurrences of each suffix\n",
    "        suffix_counts = {}\n",
    "\n",
    "        # Iterate through columns in the row\n",
    "        for colname in row.index:\n",
    "            # Check if the value is greater than or equal to the threshold\n",
    "            if row[colname] >= threshold:\n",
    "                # Extract the suffix (e.g., '.2') from the column name\n",
    "                suffix = colname.split('.')[-1]\n",
    "\n",
    "                # Increment the count for this suffix in the dictionary\n",
    "                suffix_counts[suffix] = suffix_counts.get(suffix, 0) + 1\n",
    "\n",
    "        # Check if there are at least two occurrences of any suffix\n",
    "        if any(count >= 2 for count in suffix_counts.values()):\n",
    "            counter += 1\n",
    "\n",
    "    # Calculate the fraction of rows meeting the condition\n",
    "    total_rows = len(df)\n",
    "    fraction = 100 * round(counter / total_rows, 4)\n",
    "    \n",
    "    counter = int(counter)\n",
    "\n",
    "    return counter, fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa3f89-7156-4d32-aa82-328054108767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2a1ff-c302-4b73-a851-355e8efab97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0d7ea-3cc8-45a9-9de3-45ead45e6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates figure 4C\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "species_list = ['ax', 'bull', 'cane', 'terr', 'xen']\n",
    "sample_names = ['Tongue', 'Brain', 'Stomach', 'Intestines', 'VSkin', 'DSkin', 'Liver']\n",
    "for species in species_list:\n",
    "    # Calculate the count and fraction for the current species\n",
    "    count, fraction = count_rows_above_threshold_min1(eval(species), 0.01)  #CHANGE THIS FOR THRESHOLD 10 vs 1\n",
    "    count2, fraction2 = count_rows_above_threshold_min1(eval(species), -1)\n",
    "    \n",
    "    # Cast the count to an integer\n",
    "    count = int(count)\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    results[species] = {'Total': count, 'Fraction': fraction, 'Genome':count2}\n",
    "\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.index.name = 'Species'\n",
    "results_df[\"Total\"] = results_df[\"Total\"].astype(int)\n",
    "results_df[\"Genome\"] = results_df[\"Genome\"].astype(int)\n",
    "\n",
    "new_order = [\"Total\", \"Genome\", \"Fraction\"]\n",
    "results_df = results_df[new_order]\n",
    "\n",
    "# Print the results\n",
    "results_df\n",
    "\n",
    "# Use the custom color palette\n",
    "custom_palette = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "sns.barplot(x=['ax', 'bull', 'cane', 'terr', 'xen'], y=results_df['Fraction'], palette=custom_palette)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Fraction by Species')\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/percent_expressed_total.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100b45c-02d2-4667-9157-cd2297a776be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n",
    "unique_not_tongue[\"Total\"] = results_df[\"Total\"]\n",
    "unique_not_tongue[\"Genome\"] = results_df[\"Genome\"]\n",
    "unique_not_tongue[\"Fraction\"] = ((100*unique_not_tongue[\"Count\"]/results_df[\"Total\"]))\n",
    "unique_not_tongue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b7c68-b7e3-48eb-8be1-b0b1006eb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation, p_value = pearsonr(unique_not_tongue['Fraction'], unique_not_tongue['Genome'])\n",
    "print(correlation)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d2e1c-1801-46cb-9e0f-9a8c4b15f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates figure 4F\n",
    "# Specify the colors\n",
    "colors = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "\n",
    "# Convert the 'Fraction' and 'Genome' columns to NumPy arrays with float data type\n",
    "y_vals = np.array(unique_not_tongue['Fraction'], dtype=np.float64)\n",
    "x_vals = np.array(unique_not_tongue['Genome'], dtype=np.float64)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x_vals, y_vals, c=colors)\n",
    "# Calculate the linear regression line\n",
    "coefficients = np.polyfit(x_vals, y_vals, 1)\n",
    "slope, intercept = coefficients\n",
    "\n",
    "# Add the line of best fit\n",
    "x_vals_fit = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "y_vals_fit = slope * x_vals_fit + intercept\n",
    "plt.plot(x_vals_fit, y_vals_fit, color='black', linestyle='--')\n",
    "\n",
    "# Add the regression equation, correlation coefficient, and p-value in the lower right-hand corner\n",
    "equation = f'Y = {slope:.2f}X + {intercept:.2f}'\n",
    "correlation_text = f'Correlation: {correlation:.2f}\\nP-value: {p_value:.4f}'\n",
    "plt.text(0.9, 0.1, f'{equation}\\n{correlation_text}', transform=plt.gca().transAxes, ha='right', va='bottom')\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('TAS2Rs in Genome')\n",
    "plt.ylabel('Fraction of Expressed not in Tongue')\n",
    "plt.title('Correlation between TAS2R Repetoire and Fraction Expresed but not in Tongue')\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/corr_TAS2Rnum_expEO.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ff28b-9465-4658-956f-7f5ed41ba691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"Fraction\" column from results_df\n",
    "fraction_data = results_df['Fraction']\n",
    "\n",
    "# Create a barplot\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set the figure size\n",
    "plt.bar(range(len(fraction_data)), fraction_data)\n",
    "plt.xlabel(\"Species\")  # Replace with your desired label\n",
    "plt.ylabel(\"Percent of receptors that are expressed\")  # Replace with your desired label\n",
    "plt.title(\"Percent of receptors that are expressed in at least one tissue\")  # Replace with your desired title\n",
    "plt.xticks(range(len(fraction_data)), results_df.index, rotation=90)  # Optional: Set x-axis labels\n",
    "\n",
    "plt.tight_layout()  # Optional: Adjust layout\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/percent_expressed.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93383b03-b472-4d9a-893b-0ef366b97122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold_counts_and_avg(ax, threshold_counts, avg_colnames=None):\n",
    "    # Create a DataFrame to store the results\n",
    "    ax_threshold_counts = pd.DataFrame(columns=ax.columns)\n",
    "\n",
    "    # Iterate through columns in 'ax' DataFrame\n",
    "    for col in ax.columns:\n",
    "        # Count the number of rows greater than the threshold for each column\n",
    "        count = (ax[col] > threshold_counts).sum()\n",
    "        \n",
    "        # Add the count as a column in 'ax_threshold_counts'\n",
    "        ax_threshold_counts[col] = [count]\n",
    "\n",
    "    # Create a DataFrame to store the averages\n",
    "    ax_threshold_avg = pd.DataFrame()\n",
    "\n",
    "    # Define default average column names if not provided\n",
    "    if avg_colnames is None:\n",
    "        avg_colnames = [f\"avg.{val}\" for val in range(1, 8)]\n",
    "\n",
    "    # Iterate through values .1 to .7\n",
    "    for val in range(1, 8):\n",
    "        # Filter columns that end with \".val\"\n",
    "        filtered_cols = [col for col in ax_threshold_counts.columns if col.endswith(f\".{val}\")]\n",
    "        \n",
    "        # Calculate the average for filtered columns and add to 'ax_threshold_avg'\n",
    "        avg_colname = avg_colnames[val - 1] if val <= len(avg_colnames) else f\"avg.{val}\"\n",
    "        ax_threshold_avg[avg_colname] = ax_threshold_counts[filtered_cols].mean(axis=1)\n",
    "\n",
    "    return ax_threshold_counts, ax_threshold_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4bf97-aaba-49cd-aa2c-951080bf74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.001\n",
    "ax_threshold_counts_1, ax_threshold_avg_1 = calculate_threshold_counts_and_avg(ax, threshold, sample_names)\n",
    "bull_threshold_counts_1, bull_threshold_avg_1 = calculate_threshold_counts_and_avg(bull, threshold, sample_names)\n",
    "cane_threshold_counts_1, cane_threshold_avg_1 = calculate_threshold_counts_and_avg(cane, threshold, sample_names)\n",
    "terr_threshold_counts_1, terr_threshold_avg_1 = calculate_threshold_counts_and_avg(terr, threshold, sample_names)\n",
    "xen_threshold_counts_1, xen_threshold_avg_1 = calculate_threshold_counts_and_avg(xen, threshold, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd35e5-3cfe-43fc-a91e-9d8fa313ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "ax_threshold_counts_10, ax_threshold_avg_10 = calculate_threshold_counts_and_avg(ax, threshold, sample_names)\n",
    "bull_threshold_counts_10, bull_threshold_avg_10 = calculate_threshold_counts_and_avg(bull, threshold, sample_names)\n",
    "cane_threshold_counts_10, cane_threshold_avg_10 = calculate_threshold_counts_and_avg(cane, threshold, sample_names)\n",
    "terr_threshold_counts_10, terr_threshold_avg_10 = calculate_threshold_counts_and_avg(terr, threshold, sample_names)\n",
    "xen_threshold_counts_10, xen_threshold_avg_10 = calculate_threshold_counts_and_avg(xen, threshold, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d7932-1c9e-41e7-8fd7-b9ea75b8a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "ax_threshold_counts_100, ax_threshold_avg_100 = calculate_threshold_counts_and_avg(ax, threshold, sample_names)\n",
    "bull_threshold_counts_100, bull_threshold_avg_100 = calculate_threshold_counts_and_avg(bull, threshold, sample_names)\n",
    "cane_threshold_counts_100, cane_threshold_avg_100 = calculate_threshold_counts_and_avg(cane, threshold, sample_names)\n",
    "terr_threshold_counts_100, terr_threshold_avg_100 = calculate_threshold_counts_and_avg(terr, threshold, sample_names)\n",
    "xen_threshold_counts_100, xen_threshold_avg_100 = calculate_threshold_counts_and_avg(xen, threshold, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389e978-d429-4eef-b95a-c24d73ddca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_data = {\n",
    "    'ax': ax_threshold_avg_1,\n",
    "    'bull': bull_threshold_avg_1,\n",
    "    'cane': cane_threshold_avg_1,\n",
    "    'terr': terr_threshold_avg_1,\n",
    "    'xen': xen_threshold_avg_1\n",
    "}\n",
    "\n",
    "# Concatenate the DataFrames vertically to create a new DataFrame\n",
    "combined_threshold_avg_1 = pd.concat(species_data.values(), keys=species_data.keys(), axis=0)\n",
    "combined_threshold_avg_1.reset_index(level=0, inplace=True)\n",
    "combined_threshold_avg_1.rename(columns={'level_0': 'Species'}, inplace=True)\n",
    "\n",
    "# Set the 'Species' column as the row index\n",
    "combined_threshold_avg_1.set_index('Species', inplace=True)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "combined_threshold_avg_1 = combined_threshold_avg_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f709a2-a2ed-4d7c-9221-e6d5858db1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_threshold_avg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0092b0-3cb2-4e5b-b688-4641683b4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_all(df, output_names=None):\n",
    "    if output_names is None:\n",
    "        output_names = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "\n",
    "    # Initialize an empty DataFrame for the summary table\n",
    "    df_avg_all = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Iterate through columns 1 to 7 and calculate the average for each group of columns\n",
    "    for i, output_name in enumerate(output_names, start=1):\n",
    "        # Create a list of columns with names ending in \".i\"\n",
    "        cols_to_average = [col for col in df.columns if col.endswith(f\".{i}\")]\n",
    "        \n",
    "        # Calculate the average for each row in the selected columns\n",
    "        df_avg_all[output_name] = df[cols_to_average].mean(axis=1)\n",
    "\n",
    "    return df_avg_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe480a01-d2e6-4482-8fe2-5130b638b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each of the dataframes and store the results in separate variables\n",
    "\n",
    "sample_names = [\"tongue\", \"brain\", \"stomach\", \"intestines\", \"vskin\", \"dskin\", \"liver\"]\n",
    "ax_avg_all = average_all(ax, sample_names)\n",
    "bull_avg_all = average_all(bull, sample_names)\n",
    "cane_avg_all = average_all(cane, sample_names)\n",
    "terr_avg_all = average_all(terr, sample_names)\n",
    "xen_avg_all = average_all(xen, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f62bf-f2aa-4b7c-9eaf-cea9308931c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance = distance.euclidean(xen_avg_all[\"tongue\"], xen_avg_all[\"liver\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cd303-02c8-486a-9bdd-b971f37cf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_distance_matrix(data_frame):\n",
    "    # Get the list of tissue names (column names)\n",
    "    tissue_names = data_frame.columns\n",
    "    \n",
    "    # Initialize a square matrix with NaN values\n",
    "    num_tissues = len(tissue_names)\n",
    "    distance_matrix = np.full((num_tissues, num_tissues), np.nan)\n",
    "\n",
    "    # Iterate through tissue pairs and calculate Euclidean distances, avoiding redundancy\n",
    "    for i in range(num_tissues):\n",
    "        for j in range(i+1, num_tissues):  # Start from i+1 to avoid self-comparisons and redundancy\n",
    "            tissue1 = data_frame[tissue_names[i]]\n",
    "            tissue2 = data_frame[tissue_names[j]]\n",
    "            euclidean_dist = distance.euclidean(tissue1, tissue2)\n",
    "\n",
    "            # Store the Euclidean distance in the matrix\n",
    "            distance_matrix[i, j] = euclidean_dist\n",
    "\n",
    "    # Create a DataFrame with appropriate row and column names\n",
    "    distance_df = pd.DataFrame(distance_matrix, columns=tissue_names, index=tissue_names)\n",
    "\n",
    "    return distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f49cb4-35e3-4813-81ea-ec54a97d61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_euc = calculate_pairwise_distance_matrix(ax_avg_all)\n",
    "bull_euc = calculate_pairwise_distance_matrix(bull_avg_all)\n",
    "cane_euc = calculate_pairwise_distance_matrix(cane_avg_all)\n",
    "terr_euc = calculate_pairwise_distance_matrix(terr_avg_all)\n",
    "xen_euc = calculate_pairwise_distance_matrix(xen_avg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66940c9-ed14-443d-a445-9a6ccabc1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spearman_correlation_matrix(data_frame):\n",
    "    # Calculate the Spearman correlation coefficients\n",
    "    corr_matrix, _ = spearmanr(data_frame)\n",
    "    \n",
    "    # Convert the correlation matrix to a pandas DataFrame\n",
    "    corr_df = pd.DataFrame(corr_matrix, columns=data_frame.columns, index=data_frame.columns)\n",
    "    \n",
    "    # Create a mask to hide values below and on the diagonal\n",
    "    mask = np.triu(np.ones(corr_matrix.shape), k=1)\n",
    "    \n",
    "    # Apply the mask to the correlation matrix\n",
    "    corr_matrix = np.where(mask, corr_matrix, np.nan)\n",
    "    \n",
    "    # Convert the masked correlation matrix to a pandas DataFrame\n",
    "    corr_df = pd.DataFrame(corr_matrix, columns=data_frame.columns, index=data_frame.columns)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    return corr_df\n",
    "\n",
    "# def display_correlation_matrix(data_frame):\n",
    "#     # Calculate the correlation coefficients\n",
    "#     corr_matrix = np.corrcoef(data_frame, rowvar=False)\n",
    "    \n",
    "#     # Create a mask to hide values below and on the diagonal\n",
    "#     mask = np.triu(np.ones(corr_matrix.shape), k=1)\n",
    "    \n",
    "#     # Apply the mask to the correlation matrix\n",
    "#     corr_matrix = np.where(mask, corr_matrix, np.nan)\n",
    "    \n",
    "#     # Convert the masked correlation matrix to a pandas DataFrame\n",
    "#     corr_df = pd.DataFrame(corr_matrix, columns=data_frame.columns, index=data_frame.columns)\n",
    "    \n",
    "#     # Display the DataFrame\n",
    "#     return(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40511732-c551-407a-8b37-39c237d8cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_spear = display_spearman_correlation_matrix(ax_avg_all)\n",
    "bull_spear = display_spearman_correlation_matrix(bull_avg_all)\n",
    "cane_spear = display_spearman_correlation_matrix(cane_avg_all)\n",
    "terr_spear = display_spearman_correlation_matrix(terr_avg_all)\n",
    "xen_spear = display_spearman_correlation_matrix(xen_avg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e70a86-6162-4558-a0a8-a1930ad95b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_spear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2196d-1668-4bfa-a168-d956d4f12683",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_avg_all.to_csv('ax_for_annotation_min1.csv', index=True)\n",
    "bull_avg_all.to_csv('bull_for_annotation_min1.csv', index=True)\n",
    "cane_avg_all.to_csv('cane_for_annotation_min1.csv', index=True)\n",
    "terr_avg_all.to_csv('terr_for_annotation_min1.csv', index=True)\n",
    "xen_avg_all.to_csv('xen_for_annotation_min1.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ce620-a295-4094-ad23-4532dfffaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_01_at_least_min(df, min_rep, sample_names=None):\n",
    "    if min_rep not in [1, 2, 3]:\n",
    "        raise ValueError(\"min_rep must be an integer between 1 and 3, inclusive.\")\n",
    "    \n",
    "    if sample_names is None:\n",
    "        sample_names = [str(i) for i in range(1, 8)]\n",
    "\n",
    "    # Initialize an empty DataFrame for the output\n",
    "    df_at_least_min = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Iterate through columns 1 to 7\n",
    "    for i in range(1, 8):\n",
    "        # Create a list of columns with names ending in \".i\"\n",
    "        cols_to_check = [col for col in df.columns if col.endswith(f\".{i}\")]\n",
    "\n",
    "        # Calculate the sum of values >= 1 in the selected columns\n",
    "        sum_values_geq_1 = (df[cols_to_check] >= 0.01).sum(axis=1)  #CHANGE THIS FOR THRESHOLD 10 vs 1\n",
    "\n",
    "        # Check if there are at least min_rep columns with values >= 1\n",
    "        at_least_min = sum_values_geq_1 >= min_rep\n",
    "\n",
    "        # Add the result (0 or 1) as a column in the output DataFrame\n",
    "        df_at_least_min[sample_names[i-1]] = at_least_min.astype(int)\n",
    "\n",
    "    return df_at_least_min\n",
    "\n",
    "def create_df_1_at_least_min(df, min_rep, sample_names=None):\n",
    "    if min_rep not in [1, 2, 3]:\n",
    "        raise ValueError(\"min_rep must be an integer between 1 and 3, inclusive.\")\n",
    "    \n",
    "    if sample_names is None:\n",
    "        sample_names = [str(i) for i in range(1, 8)]\n",
    "\n",
    "    # Initialize an empty DataFrame for the output\n",
    "    df_at_least_min = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Iterate through columns 1 to 7\n",
    "    for i in range(1, 8):\n",
    "        # Create a list of columns with names ending in \".i\"\n",
    "        cols_to_check = [col for col in df.columns if col.endswith(f\".{i}\")]\n",
    "\n",
    "        # Calculate the sum of values >= 1 in the selected columns\n",
    "        sum_values_geq_1 = (df[cols_to_check] >= 0.1).sum(axis=1)  #CHANGE THIS FOR THRESHOLD 10 vs 1\n",
    "\n",
    "        # Check if there are at least min_rep columns with values >= 1\n",
    "        at_least_min = sum_values_geq_1 >= min_rep\n",
    "\n",
    "        # Add the result (0 or 1) as a column in the output DataFrame\n",
    "        df_at_least_min[sample_names[i-1]] = at_least_min.astype(int)\n",
    "\n",
    "    return df_at_least_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86f5bc-12de-4c80-9777-ee53d550c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_min_1_01 = create_df_01_at_least_min(ax,1, sample_names)\n",
    "bull_min_1_01 = create_df_01_at_least_min(bull,1, sample_names)\n",
    "cane_min_1_01 = create_df_01_at_least_min(cane,1, sample_names)\n",
    "terr_min_1_01 = create_df_01_at_least_min(terr,1, sample_names)\n",
    "xen_min_1_01 = create_df_01_at_least_min(xen,1, sample_names)\n",
    "ax_min_1_1 = create_df_1_at_least_min(ax,1, sample_names)\n",
    "bull_min_1_1 = create_df_1_at_least_min(bull,1, sample_names)\n",
    "cane_min_1_1 = create_df_1_at_least_min(cane,1, sample_names)\n",
    "terr_min_1_1 = create_df_1_at_least_min(terr,1, sample_names)\n",
    "xen_min_1_1 = create_df_1_at_least_min(xen,1, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad430de-4c61-43f2-8b2d-42e82398e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_min_2_01 = create_df_01_at_least_min(ax,2, sample_names)\n",
    "bull_min_2_01 = create_df_01_at_least_min(bull,2, sample_names)\n",
    "cane_min_2_01 = create_df_01_at_least_min(cane,2, sample_names)\n",
    "terr_min_2_01 = create_df_01_at_least_min(terr,2, sample_names)\n",
    "xen_min_2_01 = create_df_01_at_least_min(xen,2, sample_names)\n",
    "ax_min_2_1 = create_df_1_at_least_min(ax,2, sample_names)\n",
    "bull_min_2_1 = create_df_1_at_least_min(bull,2, sample_names)\n",
    "cane_min_2_1 = create_df_1_at_least_min(cane,2, sample_names)\n",
    "terr_min_2_1 = create_df_1_at_least_min(terr,2, sample_names)\n",
    "xen_min_2_1 = create_df_1_at_least_min(xen,2, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f44d29-c4ef-499a-b901-a65a69eb5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xen_min_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb3f8b-20fb-45ac-9836-38a28ae82563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax_min_3 = create_df_at_least_min(ax,3, sample_names)\n",
    "# bull_min_3 = create_df_at_least_min(bull,3, sample_names)\n",
    "# cane_min_3 = create_df_at_least_min(cane,3, sample_names)\n",
    "# terr_min_3 = create_df_at_least_min(terr,3, sample_names)\n",
    "# xen_min_3 = create_df_at_least_min(xen,3, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ac9b9-b457-471f-8ed6-707fb9bf3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_for_annotation = ax_avg_all * ax_min_2_01\n",
    "bull_for_annotation = bull_avg_all * bull_min_2_01\n",
    "cane_for_annotation = cane_avg_all * cane_min_2_01\n",
    "terr_for_annotation = terr_avg_all * terr_min_2_01\n",
    "xen_for_annotation = xen_avg_all * xen_min_2_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f150be6-11fe-41ea-aeaf-de547c6f432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax_for_annotation.to_csv('ax_for_annotation.csv', index=True)\n",
    "# bull_for_annotation.to_csv('bull_for_annotation.csv', index=True)\n",
    "# cane_for_annotation.to_csv('cane_for_annotation.csv', index=True)\n",
    "# terr_for_annotation.to_csv('terr_for_annotation.csv', index=True)\n",
    "# xen_for_annotation.to_csv('xen_for_annotation.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80fdd6-24ac-4879-80e5-dcbe87fcb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unique_expression_per_tissue(dataframe):\n",
    "    unique_expression_per_tissue = {}\n",
    "    \n",
    "    # Iterate through each tissue column\n",
    "    for tissue_column in dataframe.columns:\n",
    "        # Create a set to store genes expressed in the current tissue\n",
    "        current_tissue_genes = set(dataframe[dataframe[tissue_column] == 1].index)\n",
    "        \n",
    "        # Calculate genes uniquely expressed in the current tissue\n",
    "        unique_to_current_tissue = current_tissue_genes.copy()\n",
    "        \n",
    "        # Iterate through other tissue columns\n",
    "        for other_tissue_column in dataframe.columns:\n",
    "            if other_tissue_column == tissue_column:\n",
    "                continue  # Skip the current tissue itself\n",
    "            \n",
    "            # Remove genes expressed in other tissues from the unique set\n",
    "            other_tissue_genes = set(dataframe[dataframe[other_tissue_column] == 1].index)\n",
    "            unique_to_current_tissue -= other_tissue_genes\n",
    "        \n",
    "        unique_expression_count = len(unique_to_current_tissue)\n",
    "        unique_expression_per_tissue[tissue_column] = unique_expression_count\n",
    "    \n",
    "    return unique_expression_per_tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b52033-ac92-4aaa-9664-9268781d4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique expression counts for each species\n",
    "ax_unique_expression = calculate_unique_expression_per_tissue(ax_min_2_01)\n",
    "bull_unique_expression = calculate_unique_expression_per_tissue(bull_min_2_01)\n",
    "cane_unique_expression = calculate_unique_expression_per_tissue(cane_min_2_01)\n",
    "terr_unique_expression = calculate_unique_expression_per_tissue(terr_min_2_01)\n",
    "xen_unique_expression = calculate_unique_expression_per_tissue(xen_min_2_01)\n",
    "\n",
    "# Create a DataFrame with one row per species\n",
    "unique_per_species = pd.DataFrame([ax_unique_expression, bull_unique_expression, cane_unique_expression,\n",
    "                          terr_unique_expression, xen_unique_expression])\n",
    "\n",
    "# Reset the index to have one row per species\n",
    "unique_per_species.index = ['ax', 'bull', 'cane', 'terr', 'xen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2216c8c-0d4f-4818-89f2-71ccfed8f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage(dataframe1, dataframe2):\n",
    "    # Create a copy of the first dataframe to store percentages\n",
    "    column_sums_percentages = dataframe1.copy()\n",
    "\n",
    "    # Iterate through each row in the first dataframe\n",
    "    for index, row in dataframe1.iterrows():\n",
    "        # Get the species name from the index of the first dataframe\n",
    "        species = index\n",
    "        \n",
    "        # Check if the species exists in the indices of the second dataframe\n",
    "        if species in dataframe2.index:\n",
    "            # Find the corresponding row in the second dataframe using loc\n",
    "            species_row = dataframe2.loc[species]\n",
    "\n",
    "            # Extract the 'total' value from the second dataframe\n",
    "            total_temp = species_row['Total'] #set to Total for calculating unique genes\n",
    "\n",
    "            # Calculate the percentages for each column in the first dataframe\n",
    "            column_sums_percentages.loc[index] = ((row / total_temp) * 100).round(2)\n",
    "        else:\n",
    "            # Handle the case where the species is not found in the second dataframe\n",
    "            print(f\"Species '{species}' not found in results_df\")\n",
    "\n",
    "    return column_sums_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919dd50-a271-4dd3-a4a3-9571b351d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_per_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72168764-9391-47aa-9613-2e408ec17e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e85d82-2a47-4fc0-90a0-dee1f5e6504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_percentage = calculate_percentage(unique_per_species, results_df)  #be sure to change calculate_percentage to look at total, not genome\n",
    "unique_percentage.iloc[:, 1:7+1].sum(axis=1)\n",
    "unique_percentage['Total'] = unique_percentage.sum(axis=1)\n",
    "unique_percentage['Total_EO'] = unique_percentage.iloc[:, 2:7].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851f25e-4b5a-43dc-897b-bd97bd26f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa31db5-0c07-4e9c-8e69-6dfe6d78a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "unique_percentage2 = unique_percentage.drop(columns=['Total'])\n",
    "ax1 = unique_percentage2.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Stacked Bar Plot of Tissue Percentage for Each Species')\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "\n",
    "# Display the plot\n",
    "\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/percent_unique_bytissue_stacked.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58beeccc-b6ef-419d-b5b5-41e651db683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure 4E\n",
    "# Extract the \"Fraction\" column from results_df\n",
    "fraction_data = unique_percentage['Total']\n",
    "\n",
    "# Create a barplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Use the custom color palette\n",
    "custom_palette = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "sns.barplot(x=['ax', 'bull', 'cane', 'terr', 'xen'], y=fraction_data, palette=custom_palette)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Percent of receptors that are expressed\")\n",
    "plt.title(\"Percent of expressed receptors that are only in 1 tissue\")\n",
    "plt.xticks(range(len(['ax', 'bull', 'cane', 'terr', 'xen'])), ['ax', 'bull', 'cane', 'terr', 'xen'], rotation=0)\n",
    "\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()  # Optional: Adjust layout\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/percent_unique.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcec1e9-a1a4-434a-b9e0-c43d4d773426",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_table = calculate_percentage(combined_threshold_avg_1, results_df)  #be sure to change calculate_percentage to look at total, not genome\n",
    "percentage_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10592c4f-4a7f-469d-b7e5-be50d7b042fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98453110-4201-4bc7-98f7-7fbc7100a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates figure 4D\n",
    "# Melt the DataFrame to create the grouped barplot\n",
    "percentage_table_melted = percentage_table.reset_index().melt(id_vars='Species', var_name='Tissue', value_name='Percentage')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Use 'hue' to color bars based on 'Species' and set custom palette\n",
    "custom_palette = ['#D36027', '#35AB58', '#9374B4', '#B4DBAF', '#5AB4E5']\n",
    "sns.barplot(x='Tissue', y='Percentage', hue='Species', data=percentage_table_melted, palette=custom_palette)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Tissue')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Grouped Barplot by Tissue and Species')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Species', loc='upper right')\n",
    "plt.savefig('/lab/wengpj01/expression_pipeline/results_20231031/grouped_by_tissue_01.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd758d5-cb5a-4e39-8999-ddf46c8c962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the output file path (e.g., \"table.tex\")\n",
    "# output_file = '/lab/wengpj01/vertebrate_pipeline/20231006_run/unique_1010.tex'\n",
    "\n",
    "# # Export the DataFrame as a LaTeX table\n",
    "# with open(output_file, 'w') as f:\n",
    "#     f.write(unique_percentage.to_latex(index=True, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501961b2-7a04-4f59-a48b-0a1950b5b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of receptors that are expressed in at least two replicates\n",
    "species_data = {\n",
    "    'ax': ax_min_2,\n",
    "    'bull': bull_min_2,\n",
    "    'cane': cane_min_2,\n",
    "    'terr': terr_min_2,\n",
    "    'xen': xen_min_2\n",
    "}\n",
    "\n",
    "# Calculate the column sums for each DataFrame in species_data\n",
    "species_column_sums = {species: data.sum() for species, data in species_data.items()}\n",
    "\n",
    "# Concatenate the DataFrames vertically to create a new DataFrame\n",
    "combined_min_2 = pd.concat(species_data.values(), keys=species_data.keys(), axis=0)\n",
    "combined_min_2.reset_index(level=0, inplace=True)\n",
    "combined_min_2.rename(columns={'level_0': 'Species'}, inplace=True)\n",
    "\n",
    "# Set the 'Species' column as the row index\n",
    "combined_min_2.set_index('Species', inplace=True)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "combined_min_2 = combined_min_2.round(2)\n",
    "\n",
    "# Create summary DataFrames for column sums\n",
    "column_sums_summary = pd.DataFrame(species_column_sums).T\n",
    "column_sums_summary.index.name = 'Species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69d678-5a6b-4b18-aba7-4cf94893d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums_summary_1 = column_sums_summary\n",
    "#column_sums_summary_10 = column_sums_summary\n",
    "column_sums_summary\n",
    "#column_sums_summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e416e-e80f-40d5-a523-6311b4eef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.concat([column_sums_summary_1, results_df], axis=1)\n",
    "# # Convert 'Total' and 'Genome' columns to integers\n",
    "# merged_df['Total'] = merged_df['Total'].astype(int)\n",
    "# merged_df['Genome'] = merged_df['Genome'].astype(int)\n",
    "# # Reorder the last three columns\n",
    "# merged_df = merged_df[merged_df.columns[:-3].tolist() + ['Total', 'Genome', 'Fraction']]\n",
    "# merged_df\n",
    "\n",
    "# # # Specify the output file path (e.g., \"table.tex\")\n",
    "# # output_file = '/lab/wengpj01/vertebrate_pipeline/20231006_run/10count_raw1010.tex'\n",
    "\n",
    "# # # Export the DataFrame as a LaTeX table\n",
    "# # with open(output_file, 'w') as f:\n",
    "# #     f.write(merged_df.to_latex(index=True, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab97935-e406-4297-9a32-192275ad6b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a10924-bfae-4c7a-beca-e7c49d9b8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_column_sums(column_sums_summary, results_df):\n",
    "#     # Create an empty DataFrame to store normalized data\n",
    "#     normalized_data = pd.DataFrame(columns=column_sums_summary.columns)\n",
    "\n",
    "#     # Iterate through rows in the input DataFrame\n",
    "#     for index, row in column_sums_summary.iterrows():\n",
    "#         species = index  # The species name is the row index\n",
    "#         total = results_df.at[species, 'Total']  # Fetch the corresponding total from results_df\n",
    "#         # Normalize the row data by dividing by the total\n",
    "#         normalized_row = (100*row / total).round(2)\n",
    "#         # Append the normalized row to the result DataFrame\n",
    "#         normalized_data.loc[index] = normalized_row\n",
    "\n",
    "#     return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5110d-df16-416d-92ec-3f7b253278a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = calculate_percentage(column_sums_summary_1, results_df)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc49d9f-6d8e-4841-a00f-5499fa51702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_sums_summary_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b0406-4cc8-494e-8cc1-b2ee79ec413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d80cc7-aefc-4942-b58f-c893cc8d394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98a684-dff0-43d6-8ceb-74bb2a697e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_colocalization_matrix(species, min_rep, sample_names):\n",
    "#     # Step 1: Create the species_min_rep DataFrame\n",
    "#     species_min_rep = create_df_at_least_min(species, min_rep, sample_names)\n",
    "\n",
    "#     # Step 2: Initialize an empty colocalization matrix with row and column names matching sample_names\n",
    "#     species_coloc = pd.DataFrame(index=sample_names, columns=sample_names)\n",
    "\n",
    "#     # Step 3: Fill in the colocalization matrix\n",
    "#     for row_sample in sample_names:\n",
    "#         for col_sample in sample_names:\n",
    "#             if row_sample == col_sample:\n",
    "#                 # Diagonal elements are set to 1 (same sample)\n",
    "#                 species_coloc.at[row_sample, col_sample] = 1\n",
    "#             else:\n",
    "#                 # Create a temporary matrix with the same number of rows and sample_names as species_min_rep\n",
    "#                 temp_matrix = pd.DataFrame(index=species_min_rep.index, columns=sample_names)\n",
    "                \n",
    "#                 for \n",
    "\n",
    "#                 # Check if the value in species_min_rep[row_sample] and species_min_rep[col_sample] is 1 for all rows\n",
    "#                 temp_matrix = (species_min_rep[row_sample] == 1) & (species_min_rep[col_sample] == 1)\n",
    "\n",
    "#                 # Calculate the sum of the temporary matrix column and save it in species_coloc[row_sample, col_sample]\n",
    "#                 species_coloc.at[row_sample, col_sample] = temp_matrix.sum().sum()  # Sum over all rows\n",
    "\n",
    "#     return species_coloc\n",
    "\n",
    "# # Example usage:\n",
    "# # coloc_matrix = create_colocalization_matrix(species, min_rep=2, sample_names=[\"Tongue\", \"Brain\", \"Stomach\", \"Intestines\", \"vSkin\", \"dSkin\", \"Liver\"])\n",
    "# # This will create the colocalization matrix based on 'species' with the specified criteria and custom sample names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f99ce-7d1b-480d-9e7f-42c3c8105ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acad2af-8721-4f1b-91e7-0062d9009966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_colocalization_matrix(ax, 2, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf6590-3c55-4fb2-8b10-f135fa5aad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_species_colocalization(species_min_rep, sample_names):\n",
    "    # Create an empty DataFrame 'species_colocalization'\n",
    "    species_colocalization = pd.DataFrame(index=sample_names, columns=sample_names)\n",
    "\n",
    "    # Iterate through all combinations of sample_names as both i and j\n",
    "    for i, j in combinations(sample_names, 2):\n",
    "        # Create 'temp' DataFrame for i and j\n",
    "        temp = pd.DataFrame(index=species_min_rep.index, columns=[\"Result\"])\n",
    "        \n",
    "        # Iterate across all rows of 'species_min_rep'\n",
    "        for k, row in species_min_rep.iterrows():\n",
    "            # Check if species_min_rep[i] == 1 and species_min_rep[j] == 1 in row k\n",
    "            if row[i] == 1 and row[j] == 1:\n",
    "                temp.at[k, \"Result\"] = 1\n",
    "            else:\n",
    "                temp.at[k, \"Result\"] = 0\n",
    "        \n",
    "        # Calculate the sum of the \"Result\" column and store it in 'species_colocalization[i, j]'\n",
    "        species_colocalization.at[i, j] = temp[\"Result\"].sum()\n",
    "    \n",
    "    return species_colocalization\n",
    "\n",
    "# Example usage:\n",
    "# species_coloc_matrix = calculate_species_colocalization(species_min_rep, sample_names=[\"Tongue\", \"Brain\", \"Stomach\", \"Intestines\", \"vSkin\", \"dSkin\", \"Liver\"])\n",
    "# This will create the 'species_colocalization' DataFrame with the calculated colocalization scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185d353-28b7-4b6e-8788-a0318f5ee0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_coloc = calculate_species_colocalization(ax_min_2, sample_names)\n",
    "bull_coloc = calculate_species_colocalization(bull_min_2, sample_names)\n",
    "cane_coloc = calculate_species_colocalization(cane_min_2, sample_names)\n",
    "terr_coloc = calculate_species_colocalization(terr_min_2, sample_names)\n",
    "xen_coloc = calculate_species_colocalization(xen_min_2, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05655f-f93a-412c-a16a-1f7841eeb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_coloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66babeb9-3ed0-4949-9707-bdc90a55f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_coloc_3 = calculate_species_colocalization(ax_min_3, sample_names)\n",
    "bull_coloc_3 = calculate_species_colocalization(bull_min_3, sample_names)\n",
    "cane_coloc_3 = calculate_species_colocalization(cane_min_3, sample_names)\n",
    "terr_coloc_3 = calculate_species_colocalization(terr_min_3, sample_names)\n",
    "xen_coloc_3 = calculate_species_colocalization(xen_min_3, sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4554d-f45f-4bf6-b9b9-29bb2d5a518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'cane_coloc_num' with numeric values and empty squares for non-numeric values\n",
    "ax_coloc_num = ax_euc.applymap(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else '')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "# Define a color map for the heatmap\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap with numeric values and empty squares\n",
    "sns.heatmap(ax_coloc_num, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Names\")\n",
    "plt.ylabel(\"Sample Names\")\n",
    "plt.title(\"Axolotl Toad Colocalization Heatmap\")\n",
    "\n",
    "plt.savefig(\"/lab/wengpj01/expression_pipeline/results_20231031/ax_euc.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da5ba5-2b8a-4dda-af87-3c5dd1622f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'cane_coloc_num' with numeric values and empty squares for non-numeric values\n",
    "bull_coloc_num = bull_euc.applymap(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else '')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "# Define a color map for the heatmap\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap with numeric values and empty squares\n",
    "sns.heatmap(bull_coloc_num, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Names\")\n",
    "plt.ylabel(\"Sample Names\")\n",
    "plt.title(\"Bullfrog Toad Colocalization Heatmap\")\n",
    "\n",
    "plt.savefig(\"/lab/wengpj01/expression_pipeline/results_20231031/bull_euc.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438f336-34ad-46a1-81a6-e2cbecbacff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'cane_coloc_num' with numeric values and empty squares for non-numeric values\n",
    "cane_coloc_num = cane_euc.applymap(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else '')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "# Define a color map for the heatmap\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap with numeric values and empty squares\n",
    "sns.heatmap(cane_coloc_num, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Names\")\n",
    "plt.ylabel(\"Sample Names\")\n",
    "plt.title(\"Cane Toad Colocalization Heatmap\")\n",
    "\n",
    "plt.savefig(\"/lab/wengpj01/expression_pipeline/results_20231031/cane_euc.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8f3f6-6e00-490c-a4fd-7c64571b8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'cane_coloc_num' with numeric values and empty squares for non-numeric values\n",
    "terr_coloc_num = terr_euc.applymap(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else '')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "# Define a color map for the heatmap\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap with numeric values and empty squares\n",
    "sns.heatmap(terr_coloc_num, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Names\")\n",
    "plt.ylabel(\"Sample Names\")\n",
    "plt.title(\"Terribilis Colocalization Heatmap\")\n",
    "\n",
    "plt.savefig(\"/lab/wengpj01/expression_pipeline/results_20231031/terr_euc.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f139ef-bb36-420e-b393-48518b0ba3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'cane_coloc_num' with numeric values and empty squares for non-numeric values\n",
    "xen_coloc_num = xen_euc.applymap(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else '')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "# # Define a color map for the heatmap\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap with numeric values and empty squares\n",
    "sns.heatmap(xen_coloc_num, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Names\")\n",
    "plt.ylabel(\"Sample Names\")\n",
    "plt.title(\"Xenopus Colocalization Heatmap\")\n",
    "\n",
    "plt.savefig(\"/lab/wengpj01/expression_pipeline/results_20231031/xen_euc.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1221d-67fe-460a-88d3-5bd2ac679c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_heatmap(df, sample_name, output_path, global_min, global_max):\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a color map for the heatmap\n",
    "#     cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "    cmap = sns.color_palette(\"YlOrRd\", as_cmap=True) #YlOrRd_r\n",
    "\n",
    "    # Create the heatmap with numeric values and specified vmin and vmax\n",
    "    sns.heatmap(df, annot=False, fmt='', cmap=cmap, cbar_kws={'label': 'Spearman Coefficient'}, linewidths=0.5,\n",
    "                vmin=global_min, vmax=global_max)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Sample Names\")\n",
    "    plt.ylabel(\"Sample Names\")\n",
    "    plt.title(f\"{sample_name} Colocalization Heatmap\")\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(output_path, format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e13bf1-2671-477d-9b82-577f1e82f339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04dd06e-a769-4c57-bad4-f30efa9c940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames vertically (along rows) to create a single DataFrame\n",
    "all_coloc_num = pd.concat([ax_euc, bull_euc, cane_euc, terr_euc, xen_euc])\n",
    "\n",
    "# Find the maximum and minimum values across all DataFrames\n",
    "global_max = all_coloc_num.max().max()\n",
    "global_min = all_coloc_num.min().min()\n",
    "\n",
    "# Print or use the global_max and global_min values as needed\n",
    "print(\"Global Maximum:\", global_max)\n",
    "print(\"Global Minimum:\", global_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf31eeb-5e48-4150-850c-50f7bdfde282",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_heatmap(ax_euc, \"Axolotl\", \"/lab/wengpj01/expression_pipeline/results_20231031/ax_coloc_euc.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(bull_euc, \"Bullfrog\", \"/lab/wengpj01/expression_pipeline/results_20231031/bull_coloc_euc.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(cane_euc, \"Cane Toad\", \"/lab/wengpj01/expression_pipeline/results_20231031/cane_coloc_euc.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(terr_euc, \"Terribilis\", \"/lab/wengpj01/expression_pipeline/results_20231031/terr_coloc_euc.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(xen_euc, \"Xenopus\", \"/lab/wengpj01/expression_pipeline/results_20231031/xen_coloc_euc.pdf\", global_min, global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cb97c-9c12-459c-88a2-a58675c96d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(cane_spear > 0.3).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4a7a8-75b2-435e-b782-80e8ce6b2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xen_spear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2617a2-88bd-4a7e-a054-090c7b334703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates figure 4G\n",
    "# Concatenate the DataFrames vertically (along rows) to create a single DataFrame\n",
    "all_coloc_num = pd.concat([ax_spear, bull_spear, cane_spear, terr_spear, xen_spear])\n",
    "\n",
    "# Find the maximum and minimum values across all DataFrames\n",
    "global_max = all_coloc_num.max().max()\n",
    "global_min = all_coloc_num.min().min()\n",
    "\n",
    "# Print or use the global_max and global_min values as needed\n",
    "print(\"Global Maximum:\", global_max)\n",
    "print(\"Global Minimum:\", global_min)\n",
    "create_and_save_heatmap(ax_spear, \"Axolotl\", \"/lab/wengpj01/expression_pipeline/results_20231031/ax_spear.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(bull_spear, \"Bullfrog\", \"/lab/wengpj01/expression_pipeline/results_20231031/bull_spear.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(cane_spear, \"Cane Toad\", \"/lab/wengpj01/expression_pipeline/results_20231031/cane_spear.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(terr_spear, \"Terribilis\", \"/lab/wengpj01/expression_pipeline/results_20231031/terr_spear.pdf\", global_min, global_max)\n",
    "create_and_save_heatmap(xen_spear, \"Xenopus\", \"/lab/wengpj01/expression_pipeline/results_20231031/xen_spear.pdf\", global_min, global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54980f2c-44f0-4f51-99a0-2043112806df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create and save a log-scaled heatmap\n",
    "def create_and_save_log_heatmap(df, sample_name, output_path, global_min, global_max):\n",
    "    # Apply a logarithmic transformation to the data (avoiding zero values)\n",
    "    df_log = np.log1p(df)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a color map suitable for log-scaled data (e.g., 'YlGnBu')\n",
    "    cmap = 'YlGnBu'\n",
    "\n",
    "    # Create the heatmap with log-scaled values and specified vmin and vmax\n",
    "    sns.heatmap(df_log, annot=True, fmt='.2f', cmap=cmap, cbar_kws={'label': 'Log-Colocalization Score'},\n",
    "                linewidths=0.5, vmin=np.log1p(global_min), vmax=np.log1p(global_max))\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Sample Names\")\n",
    "    plt.ylabel(\"Sample Names\")\n",
    "    plt.title(f\"{sample_name} Log-Colocalization Heatmap\")\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(output_path, format=\"pdf\")\n",
    "\n",
    "    # Show the heatmap (optional)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2826ed-fd97-45ca-a280-1b7d3d946a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to format annotations for the heatmap\n",
    "def annotate_original_value(x, pos):\n",
    "    return f'{int(x)}'  # Display the original integer value with two decimal places\n",
    "\n",
    "# Define a function to create and save a log-scaled heatmap with original value annotations\n",
    "def create_and_save_log_heatmap(df, sample_name, output_path, global_min, global_max):\n",
    "    # Apply a logarithmic transformation to the data (avoiding zero values)\n",
    "    df_log = np.log1p(df)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a color map suitable for log-scaled data (e.g., 'YlGnBu')\n",
    "    cmap = 'YlGnBu'\n",
    "\n",
    "    # Create the heatmap with log-scaled values and specified vmin and vmax\n",
    "    sns.heatmap(df_log, annot=True, fmt='', cmap=cmap, cbar_kws={'label': 'Log-Colocalization Score'},\n",
    "                linewidths=0.5, vmin=np.log1p(global_min), vmax=np.log1p(global_max))\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Sample Names\")\n",
    "    plt.ylabel(\"Sample Names\")\n",
    "    plt.title(f\"{sample_name} Log-Colocalization Heatmap\")\n",
    "\n",
    "    # Format the annotations to display the original integer values with two decimal places\n",
    "    formatter = FuncFormatter(annotate_original_value)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(output_path, format=\"pdf\")\n",
    "\n",
    "    # Show the heatmap (optional)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31ff2e-fed5-41dc-8334-11ae01d63aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for each dataset\n",
    "create_and_save_log_heatmap(ax_coloc_num, \"Axolotl\", \"/lab/wengpj01/expression_pipeline/results_20231031/ax_coloc_log.pdf\", global_min, global_max)\n",
    "create_and_save_log_heatmap(bull_coloc_num, \"Bullfrog\", \"/lab/wengpj01/expression_pipeline/results_20231031/bull_coloc_log.pdf\", global_min, global_max)\n",
    "create_and_save_log_heatmap(cane_coloc_num, \"Cane Toad\", \"/lab/wengpj01/expression_pipeline/results_20231031/cane_coloc_log.pdf\", global_min, global_max)\n",
    "create_and_save_log_heatmap(terr_coloc_num, \"Golden Dart Frog\", \"/lab/wengpj01/expression_pipeline/results_20231031/terr_coloc_log.pdf\", global_min, global_max)\n",
    "create_and_save_log_heatmap(xen_coloc_num, \"Xenopus\", \"/lab/wengpj01/expression_pipeline/results_20231031/xen_coloc_log.pdf\", global_min, global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364afae1-4d5e-4379-8148-ea7f5c2dc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_heatmap(df, sample_name, species, output_path, global_min, global_max):\n",
    "    # Normalize the input DataFrame\n",
    "    # Calculate the normalization factor\n",
    "    for_norm = results_df.at[species, 'Total']  # Use the species name to fetch the total from results_df\n",
    "\n",
    "    # Normalize the DataFrame by dividing all values by the normalization factor\n",
    "    df = 100* df / for_norm\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a color map for the heatmap\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "\n",
    "    # Create the heatmap with numeric values and specified vmin and vmax\n",
    "    sns.heatmap(df, annot=True, fmt='.2f', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5,\n",
    "                vmin=global_min, vmax=global_max)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Sample Names\")\n",
    "    plt.ylabel(\"Sample Names\")\n",
    "    plt.title(f\"{sample_name} Colocalization Heatmap\")\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(output_path, format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c3b45-af11-4226-a1ed-dd92e872b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_and_save_heatmap(ax_coloc_num, \"Axolotl\", \"/lab/wengpj01/expression_pipeline/results_20231031/ax_coloc_norm.pdf\", 0, 1)\n",
    "create_and_save_heatmap(ax_coloc_num, \"ax\", \"ax\", \"/lab/wengpj01/expression_pipeline/results_20231031/ax_coloc_norm.pdf\", 0, 50)\n",
    "create_and_save_heatmap(bull_coloc_num, \"bull\", \"bull\", \"/lab/wengpj01/expression_pipeline/results_20231031/bull_coloc_norm.pdf\", 0, 50)\n",
    "create_and_save_heatmap(cane_coloc_num, \"cane\", \"cane\", \"/lab/wengpj01/expression_pipeline/results_20231031/cane_coloc_norm.pdf\", 0, 50)\n",
    "create_and_save_heatmap(terr_coloc_num, \"terr\", \"terr\", \"/lab/wengpj01/expression_pipeline/results_20231031/terr_coloc_norm.pdf\", 0, 50)\n",
    "create_and_save_heatmap(xen_coloc_num, \"xen\", \"xen\", \"/lab/wengpj01/expression_pipeline/results_20231031/xen_coloc_norm.pdf\", 0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc4511-4b4b-4756-94b9-b544d22c433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_heatmap(df, sample_name, species, output_path):\n",
    "    # Normalize the input DataFrame\n",
    "    # Calculate the normalization factor\n",
    "    for_norm = results_df.at[species, 'Total']  # Use the species name to fetch the total from results_df\n",
    "\n",
    "    # Normalize the DataFrame by dividing all values by the normalization factor\n",
    "    df = df / for_norm\n",
    "\n",
    "    # Determine the minimum and maximum values from the DataFrame\n",
    "    vmin = df.values.min()\n",
    "    vmax = df.values.max()\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a color map for the heatmap\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True, s=85, l=45, sep=20)\n",
    "\n",
    "    # Create the heatmap with numeric values and automatically determined vmin and vmax\n",
    "    sns.heatmap(df, annot=True, fmt='.2f', cmap=cmap, cbar_kws={'label': 'Colocalization Score'}, linewidths=0.5,\n",
    "                vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Sample Names\")\n",
    "    plt.ylabel(\"Sample Names\")\n",
    "    plt.title(f\"{sample_name} Colocalization Heatmap\")\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(output_path, format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5dc04-36da-4584-9257-b7299895c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_heatmap(xen_coloc_num, \"xen\", \"xen\", \"/lab/wengpj01/expression_pipeline/results_20231031/xen_coloc_norm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6eaa5-1b59-41c1-ab15-9e340158720b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d0d8e-2397-4fca-8e89-02f48f6ec5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e990c-fd1b-4cb6-a855-16392ebbbb66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
